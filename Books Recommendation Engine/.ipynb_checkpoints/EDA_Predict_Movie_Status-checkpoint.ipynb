{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis of Movies Database and Predicting Movies Successful or Not\n",
    "\n",
    "## <center> <font color = red> RECOMMENDER SYSTEM </center>\n",
    "\n",
    "<img src = 'https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true' width=\"240\" height=\"360\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Problem Statement](#section1)<br>\n",
    "2. [Data Loading and Description](#section2)\n",
    "3. [Data Profiling](#section3)\n",
    "    - 3.1 [Understanding the Dataset](#section301)<br/>\n",
    "    - 3.2 [Pre Profiling](#section302)<br/>\n",
    "    - 3.3 [Preprocessing](#section303)<br/>\n",
    "    - 3.4 [Post Profiling](#section304)<br/>\n",
    "4. [Questions](#section4)\n",
    "    - 4.1 [How many feature play a key role in deciding what the final score?](#section401)<br/>\n",
    "    - 4.2 [What type of correlation between features to predict match score](#section402)<br/>\n",
    "5. [Preparing X (independent features) for the model building.](#section5)\n",
    "    - 5.1 [Check for the type and shape of X.](#section501)<br/>\n",
    "6. [Extract y (dependent variable) for model building.](#section5)\n",
    "    - 6.1 [Check for the type and shape of y.](#section601)<br/>\n",
    "7. [Split the value of X and y into train and test datasets](#section7)\n",
    "8. [Features Scaling](#section8)\n",
    "9. [Check the shape of X and y of train dataset.](#section9)\n",
    "10. [Check the shape of X and y of test dataset.](#section10)\n",
    "11. [Classification.](#section11)\n",
    "    - 11.1 [Test Classifiers.](#section1101)<br/>\n",
    "    - 11.2 [Principal Component Analysis.](#section1102)<br/>\n",
    "    - 11.3 [Using One hot encoding.](#section1103)<br/>\n",
    "12. [Evaluate the model](#section12)  \n",
    "13. [Conclusions](#section13)<br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that presenting product to users with the most relevant users’ needs & preferences is an important task for any company to fulfill. To do this properly, we need to be able to extract their preferences from available raw data\n",
    "\n",
    "Deducing interpretations from available raw data can be tricky, because to succeed we need to:\n",
    "\n",
    "- __Understand what the users’ needs are__: We will typically only have very limited, implicit data of what a user might be interested in. For instance, Netflix needs to infer their users’ preferences of movies based on the movies they have watched previously. The users won’t explicitly tell Netflix what they like.\n",
    "\n",
    "- __Prioritise all matches__: Even if a company like Netflix is able to satisfactorily model user preferences in movies, they still have a big problem: There are >50,000 movies out there of which thousands may fit with the user’s preferences. Which movies should Netflix recommend first?\n",
    "\n",
    "\n",
    "A Recommender System employs a statistical algorithm that seeks to predict users' preferences for a particular entity, based on the similarity between the entities or similarity between the users that previously rated those entities. The intuition is that similar types of users are likely to have similar ratings for a set of entities.\n",
    "\n",
    "We will analyse the two datasets in order to get all the informations about the users' preferences and movie to understand what the users’ needs and prioritise all matches. \n",
    "\n",
    "__In the end we will present product to users with the most relevant their’s needs & preferences using machine learning algorithms based on past data, Visualizations, Perspectives, etc.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to analyze two datasets, tmdb_5000_credits and tmdb_5000_movies.\n",
    "\n",
    "First one contains 4803 observations with the following columns:\n",
    "- movie_id\n",
    "- title\n",
    "- cast\n",
    "- crew\n",
    "\n",
    "Second one contains 4803 observations with the following columns:\n",
    "- budget, genres, homepage, id, keywords, original_language\n",
    "- original_title, overview, popularity, production_companies\n",
    "- production_countries, release_date, revenue, runtime\n",
    "- spoken_languages, status, tagline, title, vote_average, vote_count\n",
    "\n",
    "We will merge the two datasets in order to get all the informations about the actors and the directors of their relative movie.\n",
    "\n",
    "The main problem with this dataset is the .json format. Many columns in the dataset are in json format, therefore cleaning the dataset was the main challenge. For people who don't know about JSON(JavaScript Object Notation), it is basically a syntax for storing and exchanging data between two computers. It is mainly in a key:value format, and is embedded into a string.\n",
    "\n",
    "The datasets comprises of __4803 observations of 2 and 19 columns respetively__. Below is a table showing names of all the columns and their description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name             | Description                                             |\n",
    "| ------------------      |:-------------                                          :| \n",
    "| movie_id                | Identity of the moview                                  | \n",
    "| title                   | Title of the movie                                      |  \n",
    "| cast                    | Contains information of the cast of each movie          | \n",
    "| crew                    | Contains information of the crew of each movie          |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name             | Description                                              |\n",
    "| -------------------     |:-------------                                           :| \n",
    "| id                      | Identity of the movie                                    | \n",
    "| budget                  | Movie budget                                             |  \n",
    "| genres                  | Category of Movies                                       | \n",
    "| homepage                | Homepage of the movies                                   |   \n",
    "| keywords                | Information about movie                                  |\n",
    "| original_language       | Original language of movie                               |\n",
    "| original_title          | Original title of movie                                  |\n",
    "| overview                | Overview about movie                                     | \n",
    "| popularity              | Popularity of the movie                                  |\n",
    "| production_companies    | Production companies                                     |\n",
    "| production_countries    | Production countries                                     |\n",
    "| release_date            | Release date of the movie                                |\n",
    "| revenue                 | Revenue                                                  |\n",
    "| runtime                 | Time duration in minute of the movie                     |\n",
    "| spoken_languages        | Spoken languages in the movie                            |\n",
    "| status                  | Status of the movie                                      |\n",
    "| tagline                 | Tagline of the movie                                     |\n",
    "| title                   | Movie Title                                              |\n",
    "| vote_average            | Average rating                                           |\n",
    "| vote_count              | Vote count                                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source :\n",
    "https://www.themoviedb.org/documentation/api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                 # Implemennts milti-dimensional array and matrices\n",
    "import pandas as pd                                                # For data manipulation and analysis\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt                                    # Plotting library for Python programming language and it's numerical mathematics extension NumPy\n",
    "import seaborn as sns                                              # Provides a high level interface for drawing attractive and informative statistical graphics\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Utils Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# from xgboost import XGBRegressor\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "parameters = {'n_estimators': [4, 6, 9],\n",
    "              'max_features': ['log2', 'sqrt', 'auto'],\n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10],\n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1, 5, 8]\n",
    "              }\n",
    "\n",
    "def Checker(x):\n",
    "    if type(x) is pd.DataFrame:\n",
    "        return 0\n",
    "    elif type(x) is pd.Series:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def dtype(data):\n",
    "    what = Checker(data)\n",
    "    if what == 0:\n",
    "        dtypes = data.dtypes.astype('str')\n",
    "        dtypes = dtypes.str.split(r'\\d').str[0]\n",
    "    else:\n",
    "        dtypes = str(data.dtypes)\n",
    "        dtypes = re.split(r'\\d', dtypes)[0]\n",
    "    return dtypes\n",
    "\n",
    "\n",
    "def split(x, pattern):\n",
    "    '''Regex pattern finds in data and returns match. Then, it is splitted accordingly.\n",
    "        \\d = digits\n",
    "        \\l = lowercase alphabet\n",
    "        \\p = uppercase alphabet\n",
    "        \\a = all alphabet\n",
    "        \\s = symbols and punctuation\n",
    "        \\e = end of sentence\n",
    "        '''\n",
    "    pattern2 = pattern.replace('\\d', '[0-9]').replace('\\l', '[a-z]').replace('\\p', '[A-Z]').replace('\\a', '[a-zA-Z]')        .replace('\\s', '[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n",
    "\n",
    "    if dtype(x) != 'object':\n",
    "        print('Data is not string. Convert first')\n",
    "        return False\n",
    "\n",
    "    regex = re.compile(r'{}'.format(pattern))\n",
    "    if pattern == pattern2:\n",
    "        return x.str.split(pattern)\n",
    "    else:\n",
    "        return x.apply(lambda i: re.split(regex, i))\n",
    "\n",
    "\n",
    "def replace(x, pattern, with_=None):\n",
    "    '''Regex pattern finds in data and returns match. Then, it is replaced accordingly.\n",
    "        \\d = digits\n",
    "        \\l = lowercase alphabet\n",
    "        \\p = uppercase alphabet\n",
    "        \\a = all alphabet\n",
    "        \\s = symbols and punctuation\n",
    "        \\e = end of sentence\n",
    "        '''\n",
    "    if type(pattern) is list:\n",
    "        d = {}\n",
    "        for l in pattern:\n",
    "            d[l[0]] = l[1]\n",
    "        try:\n",
    "            return x.replace(d)\n",
    "        except:\n",
    "            return x.astype('str').replace(d)\n",
    "\n",
    "    pattern2 = pattern.replace('\\d', '[0-9]').replace('\\l', '[a-z]').replace('\\p', '[A-Z]').replace('\\a', '[a-zA-Z]')        .replace('\\s', '[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n",
    "\n",
    "    if dtype(x) != 'object':\n",
    "        print('Data is not string. Convert first')\n",
    "        return False\n",
    "\n",
    "    regex = re.compile(r'{}'.format(pattern))\n",
    "    if pattern == pattern2:\n",
    "        return x.str.replace(pattern, with_)\n",
    "    else:\n",
    "        return x.apply(lambda i: re.sub(regex, with_, i))\n",
    "\n",
    "\n",
    "def hcat(*columns):\n",
    "    cols = []\n",
    "    for c in columns:\n",
    "        if c is None:\n",
    "            continue\n",
    "        if type(c) in (list, tuple):\n",
    "            for i in c:\n",
    "                if type(i) not in (pd.DataFrame, pd.Series):\n",
    "                    cols.append(pd.Series(i))\n",
    "                else:\n",
    "                    cols.append(i)\n",
    "        elif type(c) not in (pd.DataFrame, pd.Series):\n",
    "            cols.append(pd.Series(c))\n",
    "        else:\n",
    "            cols.append(c)\n",
    "    return pd.concat(cols, 1)\n",
    "\n",
    "\n",
    "def parse_col_json(df, column, key, nested):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        column: string\n",
    "            name of the column to be processed.\n",
    "        key: string\n",
    "            name of the dictionary key which needs to be extracted\n",
    "    \"\"\"\n",
    "    import json\n",
    "    for index, i in zip(df.index, df[column].apply(json.loads)):\n",
    "        list1 = []\n",
    "        males = []\n",
    "        females = []\n",
    "\n",
    "        for j in range(len(i)):\n",
    "            if nested:\n",
    "                if not(((i[j][\"department\"] == \"Directing\") and (i[j][\"job\"] == \"Director\"))):\n",
    "                    continue\n",
    "            name = i[j][key]\n",
    "            if \",\" in name:\n",
    "                name = name.replace(\",\", \" \")\n",
    "            if \" \" in name:\n",
    "                name = name.replace(\" \", \"_\")\n",
    "            list1.append(name)\n",
    "            if column==\"cast\":\n",
    "                if i[j][\"gender\"] == 1:\n",
    "                    females.append(name)\n",
    "                elif i[j][\"gender\"] == 2:\n",
    "                    males.append(name)\n",
    "        df.loc[index, column] = str(list1)\n",
    "        if column==\"cast\":\n",
    "            df.loc[index, \"actors\"] = str(males)\n",
    "            df.loc[index, \"actress\"] = str(females)\n",
    "\n",
    "def counts_elements(df, columns):\n",
    "    d = defaultdict(Counter)\n",
    "    for column in columns:\n",
    "        for el in df[column]:\n",
    "            l = eval(str(el))\n",
    "            for x in l:\n",
    "                d[column][x] += 1\n",
    "    return d\n",
    "\n",
    "def counts_vectorized(df, col, min=1, vocabulary=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda x: x.split(\n",
    "        \",\"), min_df=min, vocabulary=vocabulary)\n",
    "    #analyze = vectorizer.build_analyzer()\n",
    "    #f = analyze(succ_movies.cast.iloc[0].strip(\"[]\"))\n",
    "    data = [x.strip(\"[]\") for x in df[col]]\n",
    "    #analyze([\"ciao, mamma, come, stai_oggi\", \"ehi, mamma, stai_oggi, cane\"])\n",
    "    vectorizer.fit(data)\n",
    "    counts = pd.DataFrame(vectorizer.transform(data).toarray())\n",
    "    counts.columns = [x.replace(\"'\", \"\")\n",
    "                      for x in vectorizer.get_feature_names()]\n",
    "    return counts\n",
    "\n",
    "\n",
    "def simplify(df, col, bins, group_names):\n",
    "    df[col] = df[col].fillna(-0.5)\n",
    "    categories = pd.cut(df[col], bins, labels=group_names)\n",
    "    df[col] = categories\n",
    "\n",
    "\n",
    "def encode_features(df):\n",
    "    features = ['year', 'runtime']\n",
    "\n",
    "    for feature in features:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le = le.fit(df[feature])\n",
    "        df[feature] = le.transform(df[feature])\n",
    "    return df\n",
    "\n",
    "\n",
    "def testClassifier(clf, name, dict):\n",
    "    y_pred = []\n",
    "    if name == \"Gradient Boosting\":\n",
    "        y_pred = clf.fit(X_train, y_train.values.ravel(), early_stopping_rounds=5,\n",
    "             eval_set=[(X_test, y_test)], verbose=False).predict(X_test)\n",
    "        y_pred = [round(value) for value in y_pred]\n",
    "    else:\n",
    "        clf = clf.fit(X_train, y_train.values.ravel())\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "    # Compute confusion matrix\n",
    "    CM = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    print(\"{} Scores:\\n\".format(name))\n",
    "    print(\"Accuracy: {0:.2f} %\\nPrecision: {1:.2f} %\\nRecall: {2:.2f} %\\nFall out: {3:.2f} %\\nFalse Negative Rate: {4:.2f} %\\n\\n\"\n",
    "        .format(ACC.round(4)*100.0,PPV.round(4)*100.0,TPR.round(4)*100.0,FDR.round(4)*100.0,FNR.round(4)*100.0))\n",
    "\n",
    "    dict[\"classifier\"].append(name)\n",
    "    dict[\"accuracy\"].append(ACC.round(4)*100.0)\n",
    "    dict[\"fallout\"].append(FDR.round(4)*100.0)\n",
    "    dict[\"fnr\"].append(FNR.round(4)*100.0)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the upcoming sections we will first __understand our dataset__ using various pandas functionalities.\n",
    "- Then with the help of __pandas profiling__ we will find which columns of our dataset need preprocessing.\n",
    "- In __preprocessing__ we will deal with erronous and missing values of columns. \n",
    "- Again we will do __pandas profiling__ to see how preprocessing have transformed our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "ratings = pd.read_csv('data/ratings.csv')\n",
    "books = pd.read_csv('data/books.csv')\n",
    "book_tags = pd.read_csv('data/book_tags.csv')\n",
    "tags = pd.read_csv('data/tags.csv')\n",
    "to_read = pd.read_csv('data/to_read.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section301></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain insights from data we must look into each aspect of it very carefully. We will start with observing few rows and columns of data both from the starting and from the end.\n",
    "\n",
    "Let us check the basic information of the dataset. The very basic information to know is the dimension of the dataset – rows and columns – that’s what we find out with the method __shape__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Dataset - Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Dataset - Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "books_data has __5976479 rows and 3 columns.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Dataset - Books Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "books_data has __5976479 rows and 3 columns.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Dataset - Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "books_data has __5976479 rows and 3 columns.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Dataset - To Read Books Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "books_data has __5976479 rows and 3 columns.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section302></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pre Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By pandas profiling, an __interactive HTML report__ gets generated which contins all the information about the columns of the dataset, like the __counts and type__ of each _column_. Detailed information about each column, __correlation between different columns__ and a sample of dataset.<br/>\n",
    "- It gives us __visual interpretation__ of each column in the data.\n",
    "- _Spread of the data_ can be better understood by the distribution plot. \n",
    "- _Grannular level_ analysis of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(ratings)\n",
    "profile.to_file(outputfile=\"BeforePreprocessing/ratings_data_before_preprocessing.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(books)\n",
    "profile.to_file(outputfile=\"BeforePreprocessing/books_data_before_preprocessing.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have done Pandas Profiling before preprocessing our dataset, so we have named the html file as __movies_data_before_preprocessing.html__ and __credits_data_before_preprocessing.html__. Take a look at the file and see what useful insight you can develop from it. <br/>\n",
    "Now we will process our data to better understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section303></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dealing with missing values<br/>\n",
    "    - Remove duplicates.\n",
    "    - Drop columns which is not much useful\n",
    "    - Split the year from the release date\n",
    "    - Replacing all the zeros from revenue and budget cols.\n",
    "    - Dropping all the rows with na in the columns mentioned above in the list.\n",
    "    - Filter records from both table and merge into single table.\n",
    "    - Parse JSON values from column, which can be use to predict movies rating\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove duplicates.**\n",
    "\n",
    "As observed from the dataframe above, some columns contain unneccessary information and some having duplicates values, let's update data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.drop_duplicates(keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.drop_duplicates(keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drop columns which is not much useful__\n",
    "\n",
    "Now since there is lot of null values we have in following column, and we are not goint to use them. Hense will those from books data set\n",
    "- isbn\n",
    "- isbn13\n",
    "- original_publication_year\n",
    "- original_title\n",
    "- language_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>goodreads_book_id</th>\n",
       "      <th>best_book_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>books_count</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>work_ratings_count</th>\n",
       "      <th>work_text_reviews_count</th>\n",
       "      <th>ratings_1</th>\n",
       "      <th>ratings_2</th>\n",
       "      <th>ratings_3</th>\n",
       "      <th>ratings_4</th>\n",
       "      <th>ratings_5</th>\n",
       "      <th>image_url</th>\n",
       "      <th>small_image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2792775</td>\n",
       "      <td>272</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4780653</td>\n",
       "      <td>4942365</td>\n",
       "      <td>155254</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "      <td>https://images.gr-assets.com/books/1447303603m...</td>\n",
       "      <td>https://images.gr-assets.com/books/1447303603s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  goodreads_book_id  best_book_id  work_id  books_count  \\\n",
       "0        1            2767052       2767052  2792775          272   \n",
       "\n",
       "           authors                                    title  average_rating  \\\n",
       "0  Suzanne Collins  The Hunger Games (The Hunger Games, #1)            4.34   \n",
       "\n",
       "   ratings_count  work_ratings_count  work_text_reviews_count  ratings_1  \\\n",
       "0        4780653             4942365                   155254      66715   \n",
       "\n",
       "   ratings_2  ratings_3  ratings_4  ratings_5  \\\n",
       "0     127936     560092    1481305    2706317   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://images.gr-assets.com/books/1447303603m...   \n",
       "\n",
       "                                     small_image_url  \n",
       "0  https://images.gr-assets.com/books/1447303603s...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_col = ['isbn', 'isbn13', 'original_publication_year', 'original_title',\n",
    "               'language_code']\n",
    "books.drop(useless_col, axis=1, inplace=True)\n",
    "books.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Removal of Null values__\n",
    "\n",
    "Now since there still exists 'NaN' values in our dataframe, and these are Null values, we have to do something about them. In here, I will just do the naive thing of replacing these NaNs with zeros as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id    0\n",
       "book_id    0\n",
       "rating     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id                    0\n",
       "goodreads_book_id          0\n",
       "best_book_id               0\n",
       "work_id                    0\n",
       "books_count                0\n",
       "authors                    0\n",
       "title                      0\n",
       "average_rating             0\n",
       "ratings_count              0\n",
       "work_ratings_count         0\n",
       "work_text_reviews_count    0\n",
       "ratings_1                  0\n",
       "ratings_2                  0\n",
       "ratings_3                  0\n",
       "ratings_4                  0\n",
       "ratings_5                  0\n",
       "image_url                  0\n",
       "small_image_url            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_id      0\n",
       "tag_name    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section304></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Post Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(ratings)\n",
    "profile.to_file(outputfile=\"AfterPreprocessing/ratings_data_after_preprocessing.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(books)\n",
    "profile.to_file(outputfile=\"AfterPreprocessing/books_data_after_preprocessing.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have preprocessed the data, now the dataset doesnot contain missing values. You can compare the two reports, i.e __movies_data_after_preprocessing.html__ and __movies_data_before_preprocessing.html__.<br/>\n",
    "In __movies_data_after_preprocessing.html__ report, observations:\n",
    "- In the Dataset info, Total __Missing(%)__ = __0.0%__ \n",
    "- Number of __variables__ = __13__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section304></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Utils functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChartType:\n",
    "    bar_chart = 1\n",
    "    bar_chart_horizontal = 2\n",
    "    line_chart = 3\n",
    "    histogram_chart = 4\n",
    "    stack_chart = 5\n",
    "    scatter_chart = 6\n",
    "    area_chart = 7\n",
    "    pie_chart = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showChart(data, chart_type, xlabel, ylabel, title=None, figsize=None, axis=None):\n",
    "    '''\n",
    "    data : data frame,\n",
    "    xlabel : The label text for x axis.\n",
    "    ylabel : The label text for y axis.\n",
    "    title : The label text for title of chart.\n",
    "    figsize : tuple of integers, optional, default: None\n",
    "    axis : The axis limits to be set. Either none or all of the limits must\n",
    "    be given.\n",
    "    '''\n",
    "    # Set figure size of chart\n",
    "    if figsize != None:\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "    # Set x & y axis limit\n",
    "    if axis != None:\n",
    "        plt.axis(axis) \n",
    "\n",
    "    # Draw bar chart\n",
    "    if ChartType.bar_chart == chart_type:\n",
    "        data.plot.bar()\n",
    "    elif ChartType.bar_chart_horizontal == chart_type:\n",
    "        data.plot.barh()\n",
    "    elif ChartType.stack_chart == chart_type:\n",
    "        data.plot.bar(stacked=True)\n",
    "    elif ChartType.line_chart == chart_type:\n",
    "        data.plot.line()\n",
    "    elif ChartType.histogram_chart == chart_type:\n",
    "        data.plot.hist()\n",
    "    elif ChartType.scatter_chart == chart_type:\n",
    "        data.plot.area()\n",
    "    elif ChartType.area_chart == chart_type:\n",
    "        data.plot.area()\n",
    "    elif ChartType.pie_chart == chart_type:\n",
    "        plt.pie(data.values,\n",
    "                       labels=data.index,\n",
    "                       autopct='%1.2f', startangle=90)\n",
    "        \n",
    "#         explode = (0.2, 0, 0, 0, 0, 0)\n",
    "#         plt.explode = explode\n",
    "#         plt.autopct='%1.1f%%'\n",
    "        plt.legend(data.index, loc=\"best\")\n",
    "        plt.axis('equal')\n",
    "#         plt.pctdistance=1.1\n",
    "#         plt.labeldistance=1.2\n",
    "#         data.plot.pie()\n",
    "        \n",
    "    # Set title of chart, y & x axis\n",
    "    if title != None:\n",
    "        plt.title(title, fontsize=20)\n",
    "        \n",
    "    if xlabel != None:\n",
    "        plt.xlabel(xlabel, fontsize=10)\n",
    "\n",
    "    if ylabel != None:\n",
    "        plt.ylabel(ylabel, fontsize=10)\n",
    "\n",
    "    # Custom ticks for m axis\n",
    "    plt.tick_params(axis='x', colors='black', direction='out', length=5, width=1, labelsize='large')\n",
    "    \n",
    "    # Custom ticks for m axis\n",
    "    plt.tick_params(axis='y', colors='black', direction='in', length=5, width=1, labelsize='large')\n",
    "    \n",
    "    # Show char\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section401></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How much money does a movie need to make to be profitable?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Types of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major approaches to build recommender systems: Content-Based Filtering and Collaborative Filtering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Content-Based Filtering__:\n",
    "    In content-based filtering, the similarity between different products is calculated on the basis of the attributes of the products. For instance, in a content-based movie recommender system, the similarity between the movies is calculated on the basis of genres, the actors in the movie, the director of the movie, etc.\n",
    "\n",
    "- __Collaborative Filtering__:\n",
    "    Collaborative filtering leverages the power of the crowd. The intuition behind collaborative filtering is that if a user A likes products X and Y, and if another user B likes product X, there is a fair bit of chance that he will like the product Y as well.\n",
    "\n",
    "Take the example of a movie recommender system. Suppose a huge number of users have assigned the same ratings to movies X and Y. A new user comes who has assigned the same rating to movie X but hasn't watched movie Y yet. Collaborative filtering system will recommend him the movie Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section12></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1701></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a custom input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section18></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section13></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Samuel Jackson aka Nick Fury__ from avengers has __appeared in maximum movies__. I initially thought that Morgan Freeman might be the actor with maximum movies, but Data wins over assumptions.\n",
    "- Our **Top Director** is **Steven Spielberg**, with almost all of his production is considered a success.\n",
    "- The most used **genre** is **Drama**, although **Comedy** movies has very great aspect: **100%** of them are successful.\n",
    "- **Universal Pictures** is the most prolific company, and also the most successful.\n",
    "- As we could expect, **USA** is the most prolific and successful country, with no rivals practically.\n",
    "-  The Top Actor is **Samuel L. Jackson** with 30+ successful movies. Robert De Niro represents an interesting case, passing from 2nd position, when counting the amount of attendees, to out of top-5, if we consider attendees in successful movies only.\n",
    "\n",
    "We started our analysis with this aim: to find out the key success factors on the film industry, and to try to use that factors in order to predict if a movie will be succesful or not.\n",
    "__We found out some interesting facts, for example:__\n",
    "\n",
    "  - older movies had lower runtime\n",
    "  - budget slightly increased across the years\n",
    "  - longer movies tend to have higher votes\n",
    "  - higher budget often means higher revenue and popularity\n",
    "  \n",
    "Base on given observation, overall seems the __Random Forest classifier is the best classifier__ with considering accuracy and false positive rate (who was selected as critical measure arbitrarily) followed by __Logistic Regression and K-Nearest Neighbors__ classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load the dataset\n",
    "rating = pd.read_csv('data/ratings.csv')\n",
    "books = pd.read_csv('data/books.csv')\n",
    "book_tags = pd.read_csv('data/book_tags.csv')\n",
    "tags = pd.read_csv('data/tags.csv')\n",
    "to_read = pd.read_csv('data/to_read.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_join_DF = pd.merge(book_tags, tags, left_on='tag_id', right_on='tag_id', how='inner')\n",
    "tags_join_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(books['authors'])\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a 1-dimensional array with book titles\n",
    "titles = books['title']\n",
    "indices = pd.Series(books.index, index=books['title'])\n",
    "\n",
    "# Function that get book recommendations based on the cosine similarity score of book authors\n",
    "def authors_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:21]\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[book_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_recommendations('The Hobbit').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking book_id and the best_book_id\n",
    "books['check_ids']= np.where(books['book_id'] == books['best_book_id'],'0',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['check_ids'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book= books[['book_id','books_count','isbn','authors','original_publication_year','title','language_code','average_rating','ratings_count','small_image_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags.rename(columns={'goodreads_book_id':'book_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags=book_tags.merge(tags,on='tag_id',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped= book_tags.groupby('book_id')['tag_name'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_new=pd.merge(df_book,grouped.to_frame(), on='book_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_new.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create metadata for similarity using Author,Tags and the language\n",
    "def create_metadata(x):\n",
    "    return ''.join(x['authors'])+'  '+''.join(x['tag_name'])+'  '+''.join(str(x['language_code']))\n",
    "\n",
    "df_book_new['metadata']= df_book_new.apply(create_metadata,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_new['metadata']= df_book_new['metadata'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the similarity between two books\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer= TfidfVectorizer(stop_words='english')\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_matrix=vectorizer.fit_transform(df_book_new['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity using linear kernel\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "cos_matrix= linear_kernel(df_book_matrix, df_book_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D array for book title and indices\n",
    "\n",
    "book_indices= pd.Series(df_book_new.index,index=df_book_new['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(name,sim):\n",
    "   # indx=df_book_new.loc[df_book_new['title']==name].index\n",
    "    indx=book_indices[name]\n",
    "    sim_scores=list(enumerate(sim[indx]))\n",
    "    new=sorted(sim_scores,key=lambda x: x[1],reverse=True)\n",
    "   \n",
    "    new=new[1:11]\n",
    "    #print(new)\n",
    "    book_idx=[x[0] for x in new]\n",
    "    return (df_book_new['title'].iloc[book_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('The Hunger Games (The Hunger Games, #1)',cos_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                 # Implemennts milti-dimensional array and matrices\n",
    "import pandas as pd                                                # For data manipulation and analysis\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt                                    # Plotting library for Python programming language and it's numerical mathematics extension NumPy\n",
    "import seaborn as sns                                              # Provides a high level interface for drawing attractive and informative statistical graphics\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "rating = pd.read_csv('data/ratings.csv')\n",
    "books = pd.read_csv('data/books.csv')\n",
    "book_tags = pd.read_csv('data/book_tags.csv')\n",
    "tags = pd.read_csv('data/tags.csv')\n",
    "to_read = pd.read_csv('data/to_read.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
